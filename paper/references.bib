@online{AreWeReady,
  title = {Are {{We Ready}} for {{Autonomous Drone Racing}}? {{The UZH-FPV Drone Racing Dataset}} | {{Request PDF}}},
  url = {https://www.researchgate.net/publication/335144528_Are_We_Ready_for_Autonomous_Drone_Racing_The_UZH-FPV_Drone_Racing_Dataset},
  urldate = {2023-01-30},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/CDD84LE5/335144528_Are_We_Ready_for_Autonomous_Drone_Racing_The_UZH-FPV_Drone_Racing_Dataset.html}
}

@article{bogdanDeepCalibDeepLearning2018,
  title = {{{DeepCalib}}: A Deep Learning Approach for Automatic Intrinsic Calibration of Wide Field-of-View Cameras},
  author = {Bogdan, Oleksandr and Eckstein, Viktor and Rameau, Francois and Bazin, Jean-Charles},
  date = {2018-12-13},
  journaltitle = {Conference on Visual Media Production},
  pages = {6},
  doi = {10.1145/3278471.3278479},
  abstract = {Calibration of wide field-of-view cameras is a fundamental step for numerous visual media production applications, such as 3D reconstruction, image undistortion, augmented reality and camera motion estimation. However, existing calibration methods require multiple images of a calibration pattern (typically a checkerboard), assume the presence of lines, require manual interaction and/or need an image sequence. In contrast, we present a novel fully automatic deep learning-based approach that overcomes all these limitations and works with a single image of general scenes. Our approach builds upon the recent developments in deep Convolutional Neural Networks (CNN): our network automatically estimates the intrinsic parameters of the camera (focal length and distortion parameter) from a single input image. In order to train the CNN, we leverage the great amount of omnidirectional images available on the Internet to automatically generate a large-scale dataset composed of millions of wide field-of-view images with ground truth intrinsic parameters. Experiments successfully demonstrated the quality of our results, both quantitatively and qualitatively.},
  keywords = {notion},
  annotation = {MAG ID: 2903345161 S2ID: 781341c6a0a4d8204f92381a6022638636abbae1}
}

@inproceedings{brownCloseRangeCameraCalibration1971,
  title = {Close-{{Range Camera Calibration}}},
  author = {Brown, Duane},
  date = {1971},
  url = {https://www.semanticscholar.org/paper/Close-Range-Camera-Calibration-Brown/1150007b62a3c7dac99c2c8f85c63bfab74891af},
  urldate = {2023-01-15},
  abstract = {For highest accuracies i t i s necessary in close range photogrammetry to account for the variation of lens distortion wi th in the photographic Jield. A theory to accomplish this i s developed along wi th a practical method for calibrating radial and decenfering distortion of close-range cameras. T h i s method, the analytical plumb line method, i s applied in a n experimental investigation leading to confirmation of the validity of the theoretical development accounting for variation of distortion wi th object distance. issue. Our concern in the present paper is with EXTENSION OF MAGILL'S MODEL one specific of close-range photoMagill (1955) derived and experimentally grammetry, t h a t of camera calibration. I n verified a formula which accounts for the particular, we shall be concerned with t h e of distortion with changing focus. variation of distortion within the photoMagill's result can be expressed as follows. grammetric model. This becomes a consideraLet: tion of increasing importance a s magnification increases. T h e essence of the problem as f =focal length of lens, s=distance of object plane for which lens is pointed ou t in Brown (1962) is a s follows: focussed, \textasciitilde{} \textasciitilde{} d i \textasciitilde{} l distortion is normally calibrated at 8ro=distortion function for focus on object infinity focus. Accuracies of zk2 microns rms plane at distance s, or better for the distortion function are not Gr,=distortion function of lens for infinity difficult to obtain from a rigorous stellar calibrafocus, tion. . H \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} \textasciitilde{} , optical ray tracing theory tells =distortion function of lens for inverted us that Gaussian radial distortion is a function infinity focus (i.e., distortion, if the lens is of object distance. Thus when the focal plane is reversed so that front element becomes set for a sensibly finite object distance, it is rear element and vice versa). necessary to employ the distortion function appropriate to that distance. Actually, i t is Then the magnificati0n the lens for the ject plane a t s is * Presented a t the Symposium on Close-Range Photogrammetry, Urbana, Illinois, January 1971. ma = f / ( s f )},
  keywords = {notion}
}

@article{burriEuRoCMicroAerial2016,
  title = {The {{EuRoC}} Micro Aerial Vehicle Datasets},
  author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus and Siegwart, Roland},
  date = {2016-01-25},
  journaltitle = {The International Journal of Robotics Research},
  shortjournal = {The International Journal of Robotics Research},
  volume = {35},
  doi = {10.1177/0278364915620033},
  abstract = {This paper presents visual-inertial datasets collected on-board a micro aerial vehicle. The datasets contain synchronized stereo images, IMU measurements and accurate ground truth. The first batch of datasets facilitates the design and evaluation of visual-inertial localization algorithms on real flight data. It was collected in an industrial environment and contains millimeter accurate position ground truth from a laser tracking system. The second batch of datasets is aimed at precise 3D environment reconstruction and was recorded in a room equipped with a motion capture system. The datasets contain 6D pose ground truth and a detailed 3D scan of the environment. Eleven datasets are provided in total, ranging from slow flights under good visual conditions to dynamic flights with motion blur and poor illumination, enabling researchers to thoroughly test and evaluate their algorithms. All datasets contain raw sensor measurements, spatio-temporally aligned sensor data and ground truth, extrinsic and intrinsic calibrations and datasets for custom calibrations.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/VGSPEDTX/Burri et al. - 2016 - The EuRoC micro aerial vehicle datasets.pdf}
}

@online{Calibration250degFisheye,
  title = {Calibration of a 250deg Fisheye Lens · {{Issue}} \#242 · Ethz-Asl/Kalibr},
  url = {https://github.com/ethz-asl/kalibr/issues/242},
  urldate = {2023-01-30},
  abstract = {Hi all, my goal is to find a suitable camera model for the Entaniya Fisheye M12 250 lens and calibrate it. My first attempt was to go for the Double Sphere ds\_none model recently contributed to Kal...},
  langid = {english},
  organization = {{GitHub}},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/DRKQ5JHU/242.html}
}

@inproceedings{chenNewSubPixelDetector2005,
  title = {A {{New Sub-Pixel Detector}} for {{X-Corners}} in {{Camera Calibration Targets}}.},
  author = {Chen, Dazhi and Zhang, Guangjun},
  date = {2005-01-01},
  pages = {97--100},
  abstract = {X-corner patterns are most widely used in camera calibration. In this paper, we propose a new sub-pixel detector for X-corners, which is much simpler than the traditional sub-pixel detection algorithm. In this new algorithm, the pixel position of X-corner is firstly detected by a new operator. Then a second order Taylor polynomial describing the local intensity profile around the corner is educed. The sub-pixel position of X-corner can be determined directly by calculating the saddle point of this intensity profile. Neither preliminary intensity interpolation nor quadratic fitting of the intensity surface is necessary, which greatly reduces the computation load of the detection process. Furthermore, computer simulation results indicate that our new algorithm is slightly more accurate than the traditional algorithm.},
  keywords = {notion}
}

@inproceedings{clausRationalFunctionLens2005,
  title = {A Rational Function Lens Distortion Model for General Cameras},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Claus, D. and Fitzgibbon, A.W.},
  date = {2005-06},
  volume = {1},
  pages = {213-219 vol. 1},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2005.43},
  abstract = {We introduce a new rational function (RF) model for radial lens distortion in wide-angle and catadioptric lenses, which allows the simultaneous linear estimation of motion and lens geometry from two uncalibrated views of a 3D scene. In contrast to existing models which admit such linear estimates, the new model is not specialized to any particular lens geometry, but is sufficiently general to model a variety of extreme distortions. The key step is to define the mapping between image (pixel) coordinates and 3D rays in camera coordinates as a linear combination of nonlinear functions of the image coordinates. Like a "kernel trick", this allows a linear algorithm to estimate nonlinear models, and in particular offers a simple solution to the estimation of nonlinear image distortion. The model also yields an explicit form for the epipolar curves, allowing correspondence search to be efficiently guided by the epipolar geometry. We show results of an implementation of the RF model in estimating the geometry of a real camera lens from uncalibrated footage, and compare the estimate to one obtained using a calibration grid.},
  eventtitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  keywords = {Cameras,Geometry,Kernel,Layout,Lenses,Motion estimation,Nonlinear distortion,notion,Pixel,Radio frequency,Solid modeling},
  file = {/home/astadnik/Zotero/storage/CVBNPFQD/1467270.html}
}

@software{DeltilleDetector2023,
  title = {Deltille Detector},
  date = {2023-01-28T15:35:33Z},
  origdate = {2017-12-07T16:28:35Z},
  url = {https://github.com/facebookarchive/deltille},
  urldate = {2023-01-30},
  abstract = {Detector of partial or occluded deltille (triangular) and rectangular checkerboards in camera images},
  organization = {{Meta Archive}},
  keywords = {notion}
}

@article{devernayStraightLinesHave2001,
  title = {Straight Lines Have to Be Straight},
  author = {Devernay, Frédéric and Faugeras, Olivier},
  date = {2001-08-01},
  journaltitle = {Machine Vision and Applications},
  shortjournal = {Machine Vision and Applications},
  volume = {13},
  number = {1},
  pages = {14--24},
  issn = {1432-1769},
  doi = {10.1007/PL00013269},
  url = {https://doi.org/10.1007/PL00013269},
  urldate = {2023-01-15},
  abstract = {Most algorithms in 3D computer vision rely on the pinhole camera model because of its simplicity, whereas video optics, especially low-cost wide-angle or fish-eye lenses, generate a lot of non-linear distortion which can be critical. To find the distortion parameters of a camera, we use the following fundamental property: a camera follows the pinhole model if and only if the projection of every line in space onto the camera is a line. Consequently, if we find the transformation on the video image so that every line in space is viewed in the transformed image as a line, then we know how to remove the distortion from the image. The algorithm consists of first doing edge extraction on a possibly distorted video sequence, then doing polygonal approximation with a large tolerance on these edges to extract possible lines from the sequence, and then finding the parameters of our distortion model that best transform these edges to segments. Results are presented on real video images, compared with distortion calibration obtained by a full camera calibration method which uses a calibration grid.},
  langid = {english},
  keywords = {Key words: Camera calibration – Nonlinear distortion – Self-calibration – Camera models,notion},
  file = {/home/astadnik/Zotero/storage/W7VPJ4PS/Devernay and Faugeras - 2001 - Straight lines have to be straight.pdf}
}

@online{duisterhofTartanCalibIterativeWideAngle2022,
  title = {{{TartanCalib}}: {{Iterative Wide-Angle Lens Calibration}} Using {{Adaptive SubPixel Refinement}} of {{AprilTags}}},
  shorttitle = {{{TartanCalib}}},
  author = {Duisterhof, Bardienus P. and Hu, Yaoyu and Teng, Si Heng and Kaess, Michael and Scherer, Sebastian},
  date = {2022-10-05},
  eprint = {2210.02511},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.02511},
  url = {http://arxiv.org/abs/2210.02511},
  urldate = {2023-01-05},
  abstract = {Wide-angle cameras are uniquely positioned for mobile robots, by virtue of the rich information they provide in a small, light, and cost-effective form factor. An accurate calibration of the intrinsics and extrinsics is a critical pre-requisite for using the edge of a wide-angle lens for depth perception and odometry. Calibrating wide-angle lenses with current state-of-the-art techniques yields poor results due to extreme distortion at the edge, as most algorithms assume a lens with low to medium distortion closer to a pinhole projection. In this work we present our methodology for accurate wide-angle calibration. Our pipeline generates an intermediate model, and leverages it to iteratively improve feature detection and eventually the camera parameters. We test three key methods to utilize intermediate camera models: (1) undistorting the image into virtual pinhole cameras, (2) reprojecting the target into the image frame, and (3) adaptive subpixel refinement. Combining adaptive subpixel refinement and feature reprojection significantly improves reprojection errors by up to 26.59 \%, helps us detect up to 42.01 \% more features, and improves performance in the downstream task of dense depth mapping. Finally, TartanCalib is open-source and implemented into an easy-to-use calibration toolbox. We also provide a translation layer with other state-of-the-art works, which allows for regressing generic models with thousands of parameters or using a more robust solver. To this end, TartanCalib is the tool of choice for wide-angle calibration. Project website and code: http://tartancalib.com.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,notion},
  file = {/home/astadnik/Zotero/storage/W4B5SSDB/Duisterhof et al. - 2022 - TartanCalib Iterative Wide-Angle Lens Calibration.pdf;/home/astadnik/Zotero/storage/588XH3EM/2210.html}
}

@online{fanWideangleImageRectification2021,
  title = {Wide-Angle {{Image Rectification}}: {{A Survey}}},
  shorttitle = {Wide-Angle {{Image Rectification}}},
  author = {Fan, Jinlong and Zhang, Jing and Maybank, Stephen J. and Tao, Dacheng},
  date = {2021-12-01},
  eprint = {2011.12108},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2011.12108},
  url = {http://arxiv.org/abs/2011.12108},
  urldate = {2023-01-30},
  abstract = {Wide field-of-view (FOV) cameras, which capture a larger scene area than narrow FOV cameras, are used in many applications including 3D reconstruction, autonomous driving, and video surveillance. However, wide-angle images contain distortions that violate the assumptions underlying pinhole camera models, resulting in object distortion, difficulties in estimating scene distance, area, and direction, and preventing the use of off-the-shelf deep models trained on undistorted images for downstream computer vision tasks. Image rectification, which aims to correct these distortions, can solve these problems. In this paper, we comprehensively survey progress in wide-angle image rectification from transformation models to rectification methods. Specifically, we first present a detailed description and discussion of the camera models used in different approaches. Then, we summarize several distortion models including radial distortion and projection distortion. Next, we review both traditional geometry-based image rectification methods and deep learning-based methods, where the former formulate distortion parameter estimation as an optimization problem and the latter treat it as a regression problem by leveraging the power of deep neural networks. We evaluate the performance of state-of-the-art methods on public datasets and show that although both kinds of methods can achieve good results, these methods only work well for specific camera models and distortion types. We also provide a strong baseline model and carry out an empirical study of different distortion models on synthetic datasets and real-world wide-angle images. Finally, we discuss several potential research directions that are expected to further advance this area in the future.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,notion},
  file = {/home/astadnik/Zotero/storage/CML5PF4Q/Fan et al. - 2021 - Wide-angle Image Rectification A Survey.pdf;/home/astadnik/Zotero/storage/N3FBC4M5/2011.html}
}

@article{fischlerRandomSampleConsensus1981,
  title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  shorttitle = {Random Sample Consensus},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  date = {1981-06-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {24},
  number = {6},
  pages = {381--395},
  issn = {0001-0782},
  doi = {10.1145/358669.358692},
  url = {https://doi.org/10.1145/358669.358692},
  urldate = {2023-01-12},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
  keywords = {automated cartography,camera calibration,image matching,location determination,model fitting,notion,scene analysis},
  file = {/home/astadnik/Zotero/storage/DZYJLR7Z/Fischler and Bolles - 1981 - Random sample consensus a paradigm for model fitt.pdf}
}

@inproceedings{fitzgibbonSimultaneousLinearEstimation2001,
  title = {Simultaneous Linear Estimation of Multiple View Geometry and Lens Distortion},
  booktitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  author = {Fitzgibbon, A.W.},
  date = {2001-12},
  volume = {1},
  pages = {I-I},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2001.990465},
  abstract = {A problem in uncalibrated stereo reconstruction is that cameras which deviate from the pinhole model have to be pre-calibrated in order to correct for nonlinear lens distortion. If they are not, and point correspondence is attempted using the uncorrected images, the matching constraints provided by the fundamental matrix must be set so loose that point matching is significantly hampered. This paper shows how linear estimation of the fundamental matrix from two-view point correspondences may be augmented to include one term of radial lens distortion. This is achieved by (1) changing from the standard radial-lens model to another which (as we show) has equivalent power, but which takes a simpler form in homogeneous coordinates, and (2) expressing fundamental matrix estimation as a quadratic eigenvalue problem (QEP), for which efficient algorithms are well known. I derive the new estimator, and compare its performance against bundle-adjusted calibration-grid data. The new estimator is fast enough to be included in a RANSAC-based matching loop, and we show cases of matching being rendered possible by its use. I show how the same lens can be calibrated in a natural scene where the lack of straight lines precludes most previous techniques. The modification when the multi-view relation is a planar homography or trifocal tensor is described.},
  eventtitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  keywords = {Cameras,Eigenvalues and eigenfunctions,Geometry,Image reconstruction,Layout,Lenses,Nonlinear distortion,notion,Stereo image processing,Tensile stress,Transmission line matrix methods},
  file = {/home/astadnik/Zotero/storage/IWUYNZBB/990465.html}
}

@article{fuaLeastSquaresMinimizationConstraints2010,
  title = {Least-{{Squares Minimization Under Constraints}}},
  author = {Fua, Pascal and Varol, Aydin and Urtasun, Raquel and Salzmann, Mathieu},
  date = {2010-01-01},
  abstract = {Unconstrained Least-Squares minimization is a well-studied problem. For example, the Levenberg-Marquardt is extremely effective and numerous implementations are readily available. These algorithms are, however, not designed to perform least-squares minimization under hard constraints. This short report outlines two very simple approaches to doing this. The first relies on standard Lagrange multipliers. The second is inspired by inverse kinematics techniques.},
  keywords = {notion,Read},
  file = {/home/astadnik/Zotero/storage/5ZQU28D2/Fua et al. - 2010 - Least-Squares Minimization Under Constraints.pdf}
}

@inproceedings{fuersattelOCPADOccludedCheckerboard2016,
  title = {{{OCPAD}} — {{Occluded}} Checkerboard Pattern Detector},
  author = {Fuersattel, Peter and Deitsch, Sergiu and Placht, Simon and Balda, Michael and Maier, Andreas and Riess, Christian},
  date = {2016-03-01},
  pages = {1--9},
  doi = {10.1109/WACV.2016.7477565},
  keywords = {notion}
}

@article{garrido-juradoAutomaticGenerationDetection2014,
  title = {Automatic Generation and Detection of Highly Reliable Fiducial Markers under Occlusion},
  author = {Garrido-Jurado, S. and Muñoz-Salinas, R. and Madrid-Cuevas, F. J. and Marín-Jiménez, M. J.},
  date = {2014-06-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {47},
  number = {6},
  pages = {2280--2292},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2014.01.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320314000235},
  urldate = {2023-01-05},
  abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
  langid = {english},
  keywords = {Augmented reality,Computer vision,Fiducial marker,notion},
  file = {/home/astadnik/Zotero/storage/ADZJY45P/S0031320314000235.html}
}

@inproceedings{geigerAutomaticCameraRange2012,
  title = {Automatic Camera and Range Sensor Calibration Using a Single Shot},
  booktitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Geiger, Andreas and Moosmann, Frank and Car, Ömer and Schuster, Bernhard},
  date = {2012-05},
  pages = {3936--3943},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2012.6224570},
  abstract = {As a core robotic and vision problem, camera and range sensor calibration have been researched intensely over the last decades. However, robotic research efforts still often get heavily delayed by the requirement of setting up a calibrated system consisting of multiple cameras and range measurement units. With regard to removing this burden, we present a toolbox with web interface for fully automatic camera-to-camera and camera-to-range calibration. Our system is easy to setup and recovers intrinsic and extrinsic camera parameters as well as the transformation between cameras and range sensors within one minute. In contrast to existing calibration approaches, which often require user intervention, the proposed method is robust to varying imaging conditions, fully automatic, and easy to use since a single image and range scan proves sufficient for most calibration scenarios. Experimentally, we demonstrate that the proposed checkerboard corner detector significantly outperforms current state-of-the-art. Furthermore, the proposed camera-to-range registration method is able to discover multiple solutions in the case of ambiguities. Experiments using a variety of sensors such as grayscale and color cameras, the Kinect 3D sensor and the Velodyne HDL-64 laser scanner show the robustness of our method in different indoor and outdoor settings and under various lighting conditions.},
  eventtitle = {2012 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Calibration,Cameras,notion,Optimization,Prototypes,Robot vision systems,Robustness},
  file = {/home/astadnik/Zotero/storage/J89QSDE8/6224570.html}
}

@inproceedings{geyerUnifyingTheoryCentral2000,
  title = {A {{Unifying Theory}} for {{Central Panoramic Systems}} and {{Practical Implications}}},
  author = {Geyer, Christopher and Daniilidis, Kostas},
  date = {2000-04-28},
  doi = {10.1007/3-540-45053-X_29},
  abstract = {Omnidirectional vision systems can provide panoramic alertness in surveillance, improve navigational capabilities, and produce panoramic images for multimedia. Catadioptric realizations of omnidirectional vision combine reflective surfaces and lenses. A particular class of them, the central panoramic systems, preserve the uniqueness of the projection viewpoint. In fact, every central projection system including the well known perspective projection on a plane falls into this category. In this paper, we provide a unifying theory for all central catadioptric systems. We show that all of them are isomorphic to projective mappings from the sphere to a plane with a projection center on the perpendicular to the plane. Subcases are the stereographic projection equivalent to parabolic projection and the central planar projection equivalent to every conventional camera. We define a duality among projections of points and lines as well as among different mappings. This unification is novel and has a a significant impact on the 3D interpretation of images. We present new invariances inherent in parabolic projections and a unifying calibration scheme from one view. We describe the implied advantages of catadioptric systems and explain why images arising in central catadioptric systems contain more information than images from conventional cameras. One example is that intrinsic calibration from a single view is possible for parabolic catadioptric systems given only three lines. Another example is metric rectification using only affine information about the scene.},
  eventtitle = {{{ECCV}}},
  isbn = {978-3-540-67686-7},
  keywords = {notion}
}

@inproceedings{haDeltilleGridsGeometric2017,
  title = {Deltille {{Grids}} for {{Geometric Camera Calibration}}},
  author = {Ha, Hyowon and Perdoch, Michal and Alismail, Hatem and Kweon, Inso and Sheikh, Yaser},
  date = {2017-10-01},
  pages = {5354--5362},
  doi = {10.1109/ICCV.2017.571},
  keywords = {notion}
}

@inproceedings{hannemoseSuperaccurateCameraCalibration2019,
  title = {Superaccurate Camera Calibration via Inverse Rendering},
  author = {Hannemose, Morten and Wilm, Jakob and Frisvad, Jeppe},
  date = {2019-06-21},
  pages = {40},
  doi = {10.1117/12.2531769},
  keywords = {notion,Read},
  file = {/home/astadnik/Zotero/storage/BEB595PY/Hannemose et al. - 2019 - Superaccurate camera calibration via inverse rende.pdf}
}

@inreference{HarrisAffineRegion2022,
  title = {Harris Affine Region Detector},
  booktitle = {Wikipedia},
  date = {2022-12-27T20:08:31Z},
  url = {https://en.wikipedia.org/w/index.php?title=Harris_affine_region_detector&oldid=1129927669},
  urldate = {2023-01-30},
  abstract = {In the fields of computer vision and image analysis, the Harris affine region detector belongs to the category of feature detection. Feature detection is a preprocessing step of several algorithms that rely on identifying characteristic points or interest points so to make correspondences between images, recognize textures, categorize objects or build panoramas.},
  langid = {english},
  keywords = {notion},
  annotation = {Page Version ID: 1129927669},
  file = {/home/astadnik/Zotero/storage/J4D7K4CB/Harris_affine_region_detector.html}
}

@inproceedings{harrisCombinedCornerEdge1988,
  title = {A {{Combined Corner}} and {{Edge Detector}}},
  booktitle = {Procedings of the {{Alvey Vision Conference}} 1988},
  author = {Harris, C. and Stephens, M.},
  date = {1988},
  pages = {23.1-23.6},
  publisher = {{Alvey Vision Club}},
  location = {{Manchester}},
  doi = {10.5244/C.2.23},
  url = {http://www.bmva.org/bmvc/1988/avc-88-023.html},
  urldate = {2023-05-27},
  abstract = {The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.},
  eventtitle = {Alvey {{Vision Conference}} 1988},
  langid = {english},
  keywords = {notion}
}

@book{hartleyMultipleViewGeometry2004,
  title = {Multiple {{View Geometry}} in {{Computer Vision}}},
  author = {Hartley, Richard and Zisserman, Andrew},
  date = {2004},
  edition = {2},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511811685},
  url = {https://www.cambridge.org/core/books/multiple-view-geometry-in-computer-vision/0B6F289C78B2B23F596CAA76D3D43F7A},
  urldate = {2023-05-28},
  abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
  isbn = {978-0-521-54051-3}
}

@inproceedings{heikkilaFourstepCameraCalibration1997,
  title = {A Four-Step Camera Calibration Procedure with Implicit Image Correction},
  booktitle = {Proceedings of {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Heikkila, J. and Silven, O.},
  date = {1997-06},
  pages = {1106--1112},
  issn = {1063-6919},
  doi = {10.1109/CVPR.1997.609468},
  abstract = {In geometrical camera calibration the objective is to determine a set of camera parameters that describe the mapping between 3-D reference coordinates and 2-D image coordinates. Various methods for camera calibration can be found from the literature. However surprisingly little attention has been paid to the whole calibration procedure, i.e., control point extraction from images, model fitting, image correction, and errors originating in these stages. The main interest has been in model fitting, although the other stages are also important. In this paper we present a four-step calibration procedure that is an extension to the two-step method. There is an additional step to compensate for distortion caused by circular features, and a step for correcting the distorted image coordinates. The image correction is performed with an empirical inverse model that accurately compensates for radial and tangential distortions. Finally, a linear method for solving the parameters of the inverse model is presented.},
  eventtitle = {Proceedings of {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Calibration,Cameras,Closed-form solution,Error correction,Geometrical optics,Inverse problems,Machine vision,Mathematical model,Minimization methods,Nonlinear distortion,notion},
  file = {/home/astadnik/Zotero/storage/B454SUS6/Heikkila and Silven - 1997 - A four-step camera calibration procedure with impl.pdf;/home/astadnik/Zotero/storage/BW5885YL/609468.html}
}

@online{huDeepChArUcoDark2019,
  title = {Deep {{ChArUco}}: {{Dark ChArUco Marker Pose Estimation}}},
  shorttitle = {Deep {{ChArUco}}},
  author = {Hu, Danying and DeTone, Daniel and Chauhan, Vikram and Spivak, Igor and Malisiewicz, Tomasz},
  date = {2019-07-01},
  eprint = {1812.03247},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.03247},
  url = {http://arxiv.org/abs/1812.03247},
  urldate = {2023-01-05},
  abstract = {ChArUco boards are used for camera calibration, monocular pose estimation, and pose verification in both robotics and augmented reality. Such fiducials are detectable via traditional computer vision methods (as found in OpenCV) in well-lit environments, but classical methods fail when the lighting is poor or when the image undergoes extreme motion blur. We present Deep ChArUco, a real-time pose estimation system which combines two custom deep networks, ChArUcoNet and RefineNet, with the Perspective-n-Point (PnP) algorithm to estimate the marker's 6DoF pose. ChArUcoNet is a two-headed marker-specific convolutional neural network (CNN) which jointly outputs ID-specific classifiers and 2D point locations. The 2D point locations are further refined into subpixel coordinates using RefineNet. Our networks are trained using a combination of auto-labeled videos of the target marker, synthetic subpixel corner data, and extreme data augmentation. We evaluate Deep ChArUco in challenging low-light, high-motion, high-blur scenarios and demonstrate that our approach is superior to a traditional OpenCV-based method for ChArUco marker detection and pose estimation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/home/astadnik/Zotero/storage/5R97B9BR/Hu et al. - 2019 - Deep ChArUco Dark ChArUco Marker Pose Estimation.pdf;/home/astadnik/Zotero/storage/RCJV7RAV/1812.html}
}

@article{kannalaGenericCameraModel2006,
  title = {A {{Generic Camera Model}} and {{Calibration Method}} for {{Conventional}}, {{Wide-Angle}}, and {{Fish-Eye Lenses}}},
  author = {Kannala, Juho and Brandt, Sami},
  date = {2006-09-01},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  shortjournal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {28},
  pages = {1335--40},
  doi = {10.1109/TPAMI.2006.153},
  abstract = {Fish-eye lenses are convenient in such applications where a very wide angle of view is needed, but their use for measurement purposes has been limited by the lack of an accurate, generic, and easy-to-use calibration procedure. We hence propose a generic camera model, which is suitable for fish-eye lens cameras as well as for conventional and wide-angle lens cameras, and a calibration method for estimating the parameters of the model. The achieved level of calibration accuracy is comparable to the previously reported state-of-the-art.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/LZBMVKQ6/Kannala and Brandt - 2006 - A Generic Camera Model and Calibration Method for .pdf}
}

@inproceedings{krogiusFlexibleLayoutsFiducial2019,
  title = {Flexible {{Layouts}} for {{Fiducial Tags}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Krogius, Maximilian and Haggenmiller, Acshi and Olson, Edwin},
  date = {2019-11},
  pages = {1898--1903},
  issn = {2153-0866},
  doi = {10.1109/IROS40897.2019.8967787},
  abstract = {Fiducials are artificial features with a variety of uses in computer vision such as object tracking and localization. We propose the idea of flexible tag layouts for visual fiducial systems. In contrast to traditional square tags, flexible tag layouts allow circular, annular, or other shapes as desired. One use of layout flexibility is to increase the data density of standard square shaped tags. In addition, we describe a detector that is faster and has higher recall than both the AprilTag 2 and ArUco detectors while maintaining precision.},
  eventtitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/22F6C2RU/8967787.html}
}

@article{lecyRepresentativeLiteratureReviews2012,
  title = {Representative {{Literature Reviews Using Constrained Snowball Sampling}} and {{Citation Network Analysis}}},
  author = {Lecy, Jesse and Beatty, Kate},
  date = {2012-01-01},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Electronic Journal},
  doi = {10.2139/ssrn.1992601},
  abstract = {We present here a new method for literature reviews that utilizes citation network analysis from academic databases. Special attention is paid to the construction of the citation network in order to ensure that the sample contains key publications in the field. Keyword searches in databases can be misleading because keywords are not used consistently across publications. Snowball sampling is onerous because the sample grows at an exponential rate. Constrained snowball sampling (limiting the percentage of articles collected at each level) is presented as a feasible and effective alternative that allows for robust citation analysis using a small fraction of the data. The methodology is outlined here. Free software for the R environment is also available.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/9AY9AYHX/Lecy and Beatty - 2012 - Representative Literature Reviews Using Constraine.pdf}
}

@article{leungDetectingLocalizingGrouping2001,
  title = {Detecting, {{Localizing}} and {{Grouping Repeated Scene Elements From}} an {{Image}}},
  author = {Leung, Thomas and Malik, Jitendra},
  date = {2001-10-09},
  issn = {978-3-540-61122-6},
  doi = {10.1007/BFb0015565},
  abstract = {This paper presents an algorithm for detecting, localizing and grouping instances of repeated scene elements. The grouping is represented by a graph vhere nodes correspond to individual elements and arcs join spatially neighboring elements. Associated vith each arc is an affine map that best transforms the image patch at one location to the other. The approach ve propose consists of 4 steps: (1) detecting "interesting" elements in the image; (2) matching elements vith their neighbors and estimating the affine transform betveen them; (3) grooving the element to form a more distinctive unit; and (4) grouping the elements. The idea is analogous to tracking...},
  keywords = {notion}
}

@article{liaoDRGANAutomaticRadial2020,
  title = {{{DR-GAN}}: {{Automatic Radial Distortion Rectification Using Conditional GAN}} in {{Real-Time}}},
  author = {Liao, Kang and Lin, Chunyu and Zhao, Yao and {Yao Zhao} and Gabbouj, Moncef},
  date = {2020-03-01},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {30},
  number = {3},
  pages = {725--733},
  doi = {10.1109/tcsvt.2019.2897984},
  abstract = {Radial distortion, which severely hinders object detection and semantic recognition, frequently exists in images captured using a wide-angle lens. Correction of this distortion of images is crucial in many computer vision applications. In this paper, we present distortion rectification generative adversarial network (DR-GAN), a conditional generative adversarial network (GAN) for automatic radial DR. To the best of our knowledge, this is the first end-to-end trainable adversarial framework for radial distortion rectification. The DR-GAN trained using the proposed low-to-high perceptual loss learns the mapping relation between different structural images rather than estimating multifarious distortion parameters, while also realizing label-free training and one-stage rectification. As a benefit of one-stage rectification, the proposed method is extremely fast with the completion of rectification in real time. This is approximately 22 times faster than the state-of-the-art methods. The experimental results show that the DR-GAN achieves an excellent performance in both quantitative measure (PSNR and SSIM) and visual qualitative appearance.},
  keywords = {notion},
  annotation = {MAG ID: 2914733350 S2ID: a06e30d93ed4a305316f0da727fa7ab48550192d}
}

@inproceedings{liMultiplecameraSystemCalibration2013,
  title = {A Multiple-Camera System Calibration Toolbox Using a Feature Descriptor-Based Calibration Pattern},
  booktitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Li, Bo and Heng, Lionel and Koser, Kevin and Pollefeys, Marc},
  date = {2013-11},
  pages = {1301--1307},
  issn = {2153-0866},
  doi = {10.1109/IROS.2013.6696517},
  abstract = {This paper presents a novel feature descriptor-based calibration pattern and a Matlab toolbox which uses the specially designed pattern to easily calibrate both the intrin-sics and extrinsics of a multiple-camera system. In contrast to existing calibration patterns, in particular, the ubiquitous chessboard, the proposed pattern contains many more features of varying scales; such features can be easily and automatically detected. The proposed toolbox supports the calibration of a camera system which can comprise either normal pinhole cameras or catadioptric cameras. The calibration only requires that neighboring cameras observe parts of the calibration pattern at the same time; the observed parts may not overlap at all. No overlapping fields of view are assumed for the camera system. We show that the toolbox can easily be used to automatically calibrate camera systems.},
  eventtitle = {2013 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {Calibration,Cameras,Equations,Feature extraction,Mathematical model,Noise,notion,Robot vision systems},
  file = {/home/astadnik/Zotero/storage/ZILZHZXS/6696517.html}
}

@online{lochmanBabelCalibUniversalApproach2021,
  title = {{{BabelCalib}}: {{A Universal Approach}} to {{Calibrating Central Cameras}}},
  shorttitle = {{{BabelCalib}}},
  author = {Lochman, Yaroslava and Liepieshov, Kostiantyn and Chen, Jianhui and Perdoch, Michal and Zach, Christopher and Pritts, James},
  date = {2021-10-28},
  eprint = {2109.09704},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.09704},
  url = {http://arxiv.org/abs/2109.09704},
  urldate = {2023-01-05},
  abstract = {Existing calibration methods occasionally fail for large field-of-view cameras due to the non-linearity of the underlying problem and the lack of good initial values for all parameters of the used camera model. This might occur because a simpler projection model is assumed in an initial step, or a poor initial guess for the internal parameters is pre-defined. A lot of the difficulties of general camera calibration lie in the use of a forward projection model. We side-step these challenges by first proposing a solver to calibrate the parameters in terms of a back-projection model and then regress the parameters for a target forward model. These steps are incorporated in a robust estimation framework to cope with outlying detections. Extensive experiments demonstrate that our approach is very reliable and returns the most accurate calibration parameters as measured on the downstream task of absolute pose estimation on test sets. The code is released at https://github.com/ylochman/babelcalib.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/home/astadnik/Zotero/storage/NNAIPQ49/Lochman et al. - 2021 - BabelCalib A Universal Approach to Calibrating Ce.pdf;/home/astadnik/Zotero/storage/SHCKRVBE/2109.html}
}

@article{m.fialaFullyAutomaticCamera2005,
  title = {Fully {{Automatic Camera Calibration Using Self-Identifying Calibration Targets}} *},
  author = {{M. Fiala} and {Chang Shu}},
  date = {2005},
  abstract = {Determining camera calibration parameters is an essential step in most computer vision endeavors; it is a time-consuming task despite the availability of calibration algorithms and software. A set of point correspondences between points on the calibration target and the camera image(s) must be found, usually a manual or manually guided process. Two commonly used calibration tools are implementations of Zhang’s (OpenCV) and Tsai’s algorithms, however, these assume that the correspondences are already found. A system is presented which allows a camera to be calibrated merely by passing it in front of a panel of self-identifying patterns. This calibration scheme uses an array of ARTag fiducial markers which are detected with a high degree of confidence, each detected marker provides one or four correspondence points. The user prints out the ARTag array and moves the camera relative to the pattern, the set of correspondences is automatically determined for each camera frame, and input to the OpenCV calibration code. Experiments were performed calibrating several cameras in a short period of time with no manual intervention. This system was implemented in a program for co-planar calibration, results are shown from several calibration tests with different cameras. Experiments were performed comparing using either the four ARTag marker corners or a single marker center as correspondences, and the number of image frames necessary to calibrate a camera was explored. This ARTag based calibration system was compared to one using the OpenCV grid finder cvFindChessBoardCornerGuesses() function which also finds correspondences automatically. We show how our new ARTag based system more robustly finds the calibration pattern and how it provides more accurate intrinsic camera parameters.},
  keywords = {notion},
  annotation = {S2ID: a542daea38bc6e4a163f9db53a9c5a9d53662f1e}
}

@inproceedings{matasLocalAffineFrames2002,
  title = {Local Affine Frames for Wide-Baseline Stereo},
  booktitle = {2002 {{International Conference}} on {{Pattern Recognition}}},
  author = {Matas, J. and Obdrzalek, T. and Chum, O.},
  date = {2002-08},
  volume = {4},
  pages = {363-366 vol.4},
  issn = {1051-4651},
  doi = {10.1109/ICPR.2002.1047471},
  abstract = {A novel procedure for establishing wide-baseline correspondence is introduced. Tentative correspondences are established by matching photometrically normalised colour measurements represented in a local affine frame. The affine frames are obtained by a number of affine invariant constructions on robustly detected, maximally stable extremal regions of data-dependent shape. Several processes for local affine frame construction are proposed and proved affine covariant. The potential of the proposed approach is demonstrated on demanding wide-baseline matching problems. Correspondence between two views taken from different viewpoints and camera orientations as well as at very different scales is reliably established. For the scale change present (a factor more than 3), the zoomed-in image covers less than 10\% of the wider view.},
  eventtitle = {2002 {{International Conference}} on {{Pattern Recognition}}},
  keywords = {Application software,Cameras,Computer vision,Layout,notion,Photometry,Robustness,Shape,Signal processing,Speech processing,Stereo vision},
  file = {/home/astadnik/Zotero/storage/9HSC5U7E/Matas et al. - 2002 - Local affine frames for wide-baseline stereo.pdf}
}

@inproceedings{mayeSelfsupervisedCalibrationRobotic2013,
  title = {Self-Supervised Calibration for Robotic Systems},
  booktitle = {2013 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Maye, Jérôme and Furgale, Paul and Siegwart, Roland},
  date = {2013-06},
  pages = {473--480},
  issn = {1931-0587},
  doi = {10.1109/IVS.2013.6629513},
  abstract = {We present a generic algorithm for self calibration of robotic systems that utilizes two key innovations. First, it uses information theoretic measures to automatically identify and store novel measurement sequences. This keeps the computation tractable by discarding redundant information and allows the system to build a sparse but complete calibration dataset from data collected at different times. Second, as the full observability of the calibration parameters may not be guaranteed for an arbitrary measurement sequence, the algorithm detects and locks unobservable directions in parameter space using a truncated QR decomposition of the Gauss-Newton system. The result is an algorithm that listens to an incoming sensor stream, builds a minimal set of data for estimating the calibration parameters, and updates parameters as they become observable, leaving the others locked at their initial guess. Through an extensive set of simulated and real-world experiments, we demonstrate that our method outperforms state-of-the-art algorithms in terms of stability, accuracy, and computational efficiency.},
  eventtitle = {2013 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  keywords = {Calibration,Matrix decomposition,notion,Observability,Robot kinematics,Robot sensing systems,Vehicles},
  file = {/home/astadnik/Zotero/storage/5LAFCJK4/Maye et al. - 2013 - Self-supervised calibration for robotic systems.pdf;/home/astadnik/Zotero/storage/7I3BNB6Q/6629513.html}
}

@inproceedings{meiSingleViewPoint2007,
  title = {Single {{View Point Omnidirectional Camera Calibration}} from {{Planar Grids}}},
  booktitle = {Proceedings 2007 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Mei, Christopher and Rives, Patrick},
  date = {2007-04},
  pages = {3945--3950},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2007.364084},
  abstract = {This paper presents a flexible approach for calibrating omnidirectional single viewpoint sensors from planar grids. These sensors are increasingly used in robotics where accurate calibration is often a prerequisite. Current approaches in the field are either based on theoretical properties and do not take into account important factors such as misalignment or camera-lens distortion or over-parametrised which leads to minimisation problems that are difficult to solve. Recent techniques based on polynomial approximations lead to impractical calibration methods. Our model is based on an exact theoretical projection function to which we add well identified parameters to model real-world errors. This leads to a full methodology from the initialisation of the intrinsic parameters to the general calibration. We also discuss the validity of the approach for fish-eye and spherical models. An implementation of the method is available as OpenSource software on the author's Web page. We validate the approach with the calibration of parabolic, hyperbolic, folded mirror, wide-angle and spherical sensors.},
  eventtitle = {Proceedings 2007 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Calibration,Cameras,Equations,Lenses,Mirrors,notion,Optical distortion,Optical sensors,Polynomials,Robot sensing systems,Robot vision systems},
  file = {/home/astadnik/Zotero/storage/5SKRHVG2/Mei and Rives - 2007 - Single View Point Omnidirectional Camera Calibrati.pdf;/home/astadnik/Zotero/storage/2FN5BDR7/4209702.html}
}

@inproceedings{micusikEstimationOmnidirectionalCamera2003,
  title = {Estimation of Omnidirectional Camera Model from Epipolar Geometry},
  booktitle = {2003 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, 2003. {{Proceedings}}.},
  author = {Micusik, B. and Pajdla, T.},
  date = {2003-06},
  volume = {1},
  pages = {I-I},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2003.1211393},
  abstract = {We generalize the method of simultaneous linear estimation of multiple view geometry and lens distortion, introduced by Fitzgibbon at CVPR 2001, to an omnidirectional (angle of view larger than 180/spl deg/) camera. The perspective camera is replaced by a linear camera with a spherical retina and a nonlinear mapping of the sphere into the image plane. Unlike the previous distortion-based models, the new camera model is capable to describe a camera with an angle of view larger than 180/spl deg/ at the cost of introducing only one extra parameter. A suitable linearization of the camera model and of the epipolar constraint is developed in order to arrive at a quadratic eigenvalue problem for which efficient algorithms are known. The lens calibration is done from automatically established image correspondences only. Besides rigidity, no assumptions about the scene are made (e.g. presence of a calibration object). We demonstrate the method in experiments with Nikon FC-E8 fish-eye converter for COOLPIX digital camera. In practical situations, the proposed method allows to incorporate the new omnidirectional camera model into RANSAC - a robust estimation technique.},
  eventtitle = {2003 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, 2003. {{Proceedings}}.},
  keywords = {Calibration,Cameras,Costs,Eigenvalues and eigenfunctions,Geometry,Layout,Lenses,Nonlinear distortion,notion,Retina,Solid modeling},
  file = {/home/astadnik/Zotero/storage/S4BPRRVY/Micusik and Pajdla - 2003 - Estimation of omnidirectional camera model from ep.pdf;/home/astadnik/Zotero/storage/FUQH7UDB/1211393.html}
}

@article{ntouskosAutomaticCalibrationDigital2007,
  title = {Automatic Calibration of Digital Cameras Using Planar Chess-Board Patterns},
  author = {Ntouskos, Valsamis and Kalisperakis, Ilias and Karras, George},
  date = {2007-01-01},
  journaltitle = {Optical 3-D Measurement Techniques VIII},
  shortjournal = {Optical 3-D Measurement Techniques VIII},
  volume = {1},
  abstract = {A variety of methods for camera calibration, relying on different camera models, algorithms and a priori object information, have been reported and reviewed in literature. Use of simple 2D patterns of the chess-board type represents an interesting approach, for which several 'calibration toolboxes' are available on the Internet, requiring varying degrees of human interaction. This paper presents an automatic multi-image approach exclusively for camera calibration purposes on the assumption that the imaged pattern consists of adjacent light and dark squares of equal size. Calibration results, also based on image sets from Internet sources, are viewed as satisfactory and comparable to those from other approaches. Questions regarding the role of image configuration need further investigation.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/PYJ328B5/Ntouskos et al. - 2007 - Automatic calibration of digital cameras using pla.pdf}
}

@inproceedings{olsonAprilTagRobustFlexible2011,
  title = {{{AprilTag}}: {{A}} Robust and Flexible Visual Fiducial System},
  shorttitle = {{{AprilTag}}},
  booktitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Olson, Edwin},
  date = {2011-05},
  pages = {3400--3407},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2011.5979561},
  abstract = {While the use of naturally-occurring features is a central focus of machine perception, artificial features (fiducials) play an important role in creating controllable experiments, ground truthing, and in simplifying the development of systems where perception is not the central objective. We describe a new visual fiducial system that uses a 2D bar code style "tag", allowing full 6 DOF localization of features from a single image. Our system improves upon previous systems, incorporating a fast and robust line detection system, a stronger digital coding system, and greater robustness to occlusion, warping, and lens distortion. While similar in concept to the ARTag system, our method is fully open and the algorithms are documented in detail.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  keywords = {Detectors,Encoding,Image segmentation,notion,Payloads,Robots,Robustness,Visualization},
  file = {/home/astadnik/Zotero/storage/9WS5KLZ4/Olson - 2011 - AprilTag A robust and flexible visual fiducial sy.pdf;/home/astadnik/Zotero/storage/PWJ3P7GW/5979561.html}
}

@online{OpenCVCameraCalibration,
  title = {{{OpenCV}}: {{Camera Calibration}}},
  url = {https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html},
  urldate = {2023-01-15},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/FCKFEZB7/tutorial_py_calibration.html}
}

@online{OpenCVDetectionArUco,
  title = {{{OpenCV}}: {{Detection}} of {{ArUco Markers}}},
  url = {https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html},
  urldate = {2023-01-15},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/ZA2JGQ74/tutorial_aruco_detection.html}
}

@online{OpenCVDetectionChArUco,
  title = {{{OpenCV}}: {{Detection}} of {{ChArUco Boards}}},
  url = {https://docs.opencv.org/3.4/df/d4a/tutorial_charuco_detection.html},
  urldate = {2023-01-30},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/B5L57KGG/tutorial_charuco_detection.html}
}

@article{prittsMinimalSolversRectifying2021,
  title = {Minimal {{Solvers}} for {{Rectifying}} from {{Radially-Distorted Conjugate Translations}}},
  author = {Pritts, James and Kukelova, Zuzana and Larsson, Viktor and Lochman, Yaroslava and Chum, Ondřej},
  date = {2021-11-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {11},
  eprint = {1911.01507},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {3931--3948},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992261},
  url = {http://arxiv.org/abs/1911.01507},
  urldate = {2023-01-05},
  abstract = {This paper introduces minimal solvers that jointly solve for radial lens undistortion and affine-rectification using local features extracted from the image of coplanar translated and reflected scene texture, which is common in man-made environments. The proposed solvers accommodate different types of local features and sampling strategies, and three of the proposed variants require just one feature correspondence. State-of-the-art techniques from algebraic geometry are used to simplify the formulation of the solvers. The generated solvers are stable, small and fast. Synthetic and real-image experiments show that the proposed solvers have superior robustness to noise compared to the state of the art. The solvers are integrated with an automated system for rectifying imaged scene planes from coplanar repeated texture. Accurate rectifications on challenging imagery taken with narrow to wide field-of-view lenses demonstrate the applicability of the proposed solvers.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/home/astadnik/Zotero/storage/D6MGAKBI/Pritts et al. - 2021 - Minimal Solvers for Rectifying from Radially-Disto.pdf;/home/astadnik/Zotero/storage/5GM5B349/1911.html}
}

@inproceedings{prittsRadiallyDistortedConjugateTranslations2017,
  title = {Radially-{{Distorted Conjugate Translations}}},
  author = {Pritts, James and Kukelova, Zuzana and Larsson, Viktor and Chum, Ondrej},
  date = {2017-11-30},
  doi = {10.1109/CVPR.2018.00213},
  abstract = {This paper introduces the first minimal solvers that jointly solve for affine-rectification and radial lens distortion from coplanar repeated patterns. Even with imagery from moderately distorted lenses, plane rectification using the pinhole camera model is inaccurate or invalid. The proposed solvers incorporate lens distortion into the camera model and extend accurate rectification to wide-angle imagery, which is now common from consumer cameras. The solvers are derived from constraints induced by the conjugate translations of an imaged scene plane, which are integrated with the division model for radial lens distortion. The hidden-variable trick with ideal saturation is used to reformulate the constraints so that the solvers generated by the Grobner-basis method are stable, small and fast. The proposed solvers are used in a RANSAC-based estimator. Rectification and lens distortion are recovered from either one conjugately translated affine-covariant feature or two independently translated similarity-covariant features. Experiments confirm that RANSAC accurately estimates the rectification and radial distortion with very few iterations. The proposed solvers are evaluated against the state-of-the-art for affine rectification and radial distortion estimation.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/6VT3PV2R/Pritts et al. - 2017 - Radially-Distorted Conjugate Translations.pdf}
}

@inproceedings{prittsRectificationSegmentationCoplanar2014,
  title = {Rectification, and {{Segmentation}} of {{Coplanar Repeated Patterns}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Pritts, James and Chum, Ondrej and Matas, Jirí},
  date = {2014-06},
  pages = {2973--2980},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2014.380},
  abstract = {This paper presents a novel and general method for the detection, rectification and segmentation of imaged coplanar repeated patterns. The only assumption made of the scene geometry is that repeated scene elements are mapped to each other by planar Euclidean transformations. The class of patterns covered is broad and includes nearly all commonly seen, planar, man-made repeated patterns. In addition, novel linear constraints are used to reduce geometric ambiguity between the rectified imaged pattern and the scene pattern. Rectification to within a similarity of the scene plane is achieved from one rotated repeat, or to within a similarity with a scale ambiguity along the axis of symmetry from one reflected repeat. A stratum of constraints is derived that gives the necessary configuration of repeats for each successive level of rectification. A generative model for the imaged pattern is inferred and used to segment the pattern with pixel accuracy. Qualitative results are shown on a broad range of image types on which state-of-the-art methods fail.},
  eventtitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Cameras,Estimation,Feature extraction,homgraphy,Image segmentation,Lattices,Nonlinear distortion,notion,rectification,reflection,repeated pattern,rotation,segmentation,single-view geometry,symmetry,Vectors},
  file = {/home/astadnik/Zotero/storage/3TSGFZMS/Pritts et al. - 2014 - Rectification, and Segmentation of Coplanar Repeat.pdf;/home/astadnik/Zotero/storage/ZP77EVAK/6909776.html}
}

@article{ramalingamUnifyingModelCamera2017,
  title = {A {{Unifying Model}} for {{Camera Calibration}}},
  author = {Ramalingam, Srikumar and Sturm, Peter},
  date = {2017-07},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {39},
  number = {7},
  pages = {1309--1319},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2016.2592904},
  abstract = {This paper proposes a unified theory for calibrating a wide variety of camera models such as pinhole, fisheye, cata-dioptric, and multi-camera networks. We model any camera as a set of image pixels and their associated camera rays in space. Every pixel measures the light traveling along a (half-) ray in 3-space, associated with that pixel. By this definition, calibration simply refers to the computation of the mapping between pixels and the associated 3D rays. Such a mapping can be computed using images of calibration grids, which are objects with known 3D geometry, taken from unknown positions. This general camera model allows to represent non-central cameras; we also consider two special subclasses, namely central and axial cameras. In a central camera, all rays intersect in a single point, whereas the rays are completely arbitrary in a non-central one. Axial cameras are an intermediate case: the camera rays intersect a single line. In this work, we show the theory for calibrating central, axial and non-central models using calibration grids, which can be either three-dimensional or planar.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Calibration,Camera calibration,Cameras,cata-dioptric,Computational modeling,generic imaging model,Mirrors,non-central,notion,omni-directional,Solid modeling,Three-dimensional displays},
  file = {/home/astadnik/Zotero/storage/NWFPHAUZ/Ramalingam and Sturm - 2017 - A Unifying Model for Camera Calibration.pdf;/home/astadnik/Zotero/storage/KDD85AC7/7516654.html}
}

@online{ReceiverOperatingCharacteristic,
  title = {Receiver Operating Characteristic - {{Wikipedia}}},
  url = {https://en.wikipedia.org/wiki/Receiver_operating_characteristic},
  urldate = {2023-05-28},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/CZY3FYU5/Receiver_operating_characteristic.html}
}

@inproceedings{rehderExtendingKalibrCalibrating2016,
  title = {Extending Kalibr: {{Calibrating}} the Extrinsics of Multiple {{IMUs}} and of Individual Axes},
  shorttitle = {Extending Kalibr},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Rehder, Joern and Nikolic, Janosch and Schneider, Thomas and Hinzmann, Timo and Siegwart, Roland},
  date = {2016-05},
  pages = {4304--4311},
  doi = {10.1109/ICRA.2016.7487628},
  abstract = {An increasing number of robotic systems feature multiple inertial measurement units (IMUs). Due to competing objectives-either desired vicinity to the center of gravity when used in controls, or an unobstructed field of view when integrated in a sensor setup with an exteroceptive sensor for ego-motion estimation-individual IMUs are often mounted at considerable distance. As a result, they sense different accelerations when the platform is subjected to rotational motions. In this work, we derive a method for spatially calibrating multiple IMUs in a single estimator based on the open-source camera/IMU calibration toolbox kalibr. We further extend the toolbox to determine IMU intrinsics, enabling accurate calibration of low-cost IMUs. The results suggest that the extended estimator is capable of precisely determining these intrinsics and even of localizing individual accelerometer axes inside a commercial grade IMU to millimeter precision.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Acceleration,Accelerometers,Calibration,Cameras,Estimation,Gyroscopes,notion,Robot sensing systems},
  file = {/home/astadnik/Zotero/storage/YI8ZVNT8/7487628.html}
}

@online{ResearchRabbit,
  title = {{{ResearchRabbit}}},
  url = {https://www.researchrabbit.ai},
  urldate = {2023-01-11},
  abstract = {The most powerful discovery app ever built for researchers 🔥},
  langid = {american},
  organization = {{ResearchRabbit}},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/EQWEC3S4/www.researchrabbit.ai.html}
}

@online{rosebrockAprilTagPython2020,
  title = {{{AprilTag}} with {{Python}}},
  author = {Rosebrock, Adrian},
  date = {2020-11-02T15:00:00+00:00},
  url = {https://pyimagesearch.com/2020/11/02/apriltag-with-python/},
  urldate = {2023-01-30},
  abstract = {In this tutorial, you will learn how to perform AprilTag detection with Python and the OpenCV library. AprilTags are a type of fiducial marker. Fiducials, or more simply “markers,” are reference objects that are placed in the field of view…},
  langid = {american},
  organization = {{PyImageSearch}},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/SKCELXA4/apriltag-with-python.html}
}

@book{salzmannDeformableSurface3D2010,
  title = {Deformable {{Surface 3D Reconstruction}} from {{Monocular Images}}},
  author = {Salzmann, Mathieu and Fua, Pascal},
  date = {2010-09-20},
  journaltitle = {Synthesis Lectures on Computer Vision},
  volume = {2},
  doi = {10.2200/S00319ED1V01Y201012COV003},
  abstract = {Being able to recover the shape of 3D deformable surfaces from a single video stream would make it possible to field reconstruction systems that run on widely available hardware without requiring specialized devices. However, because many different 3D shapes can have virtually the same projection, such monocular shape recovery is inherently ambiguous. In this survey, we will review the two main classes of techniques that have proved most effective so far: The template-based methods that rely on establishing correspondences with a reference image in which the shape is already known, and non-rigid structure-from-motion techniques that exploit points tracked across the sequences to reconstruct a completely unknown shape. In both cases, we will formalize the approach, discuss its inherent ambiguities, and present the practical solutions that have been proposed to resolve them. To conclude, we will suggest directions for future research.},
  keywords = {notion,Read},
  file = {/home/astadnik/Zotero/storage/5PK8IUUB/Salzmann and Fua - 2010 - Deformable Surface 3D Reconstruction from Monocula.pdf}
}

@article{scaramuzzaFlexibleTechniqueAccurate2006,
  title = {A {{Flexible Technique}} for {{Accurate Omnidirectional Camera Calibration}} and {{Structure}} from {{Motion}}},
  author = {Scaramuzza, D. and Martinelli, A. and Siegwart, R.},
  date = {2006},
  journaltitle = {Fourth IEEE International Conference on Computer Vision Systems (ICVS'06)},
  pages = {45--45},
  publisher = {{IEEE}},
  location = {{New York, NY, USA}},
  doi = {10.1109/ICVS.2006.3},
  url = {http://ieeexplore.ieee.org/document/1578733/},
  urldate = {2023-01-30},
  abstract = {In this paper, we present a flexible new technique for single viewpoint omnidirectional camera calibration. The proposed method only requires the camera to observe a planar pattern shown at a few different orientations. Either the camera or the planar pattern can be freely moved. No a priori knowledge of the motion is required, nor a specific model of the omnidirectional sensor. The only assumption is that the image projection function can be described by a Taylor series expansion whose coefficients are estimated by solving a two-step least-squares linear minimization problem. To test the proposed technique, we calibrated a panoramic camera having a field of view greater than 200 in the vertical direction, and we obtained very good results. To investigate the accuracy of the calibration, we also used the estimated omni-camera model in a structure from motion experiment. We obtained a 3D metric reconstruction of a scene from two highly distorted omnidirectional images by using image correspondences only. Compared with classical techniques, which rely on a specific parametric model of the omnidirectional camera, the proposed procedure is independent of the sensor, easy to use, and flexible.},
  eventtitle = {Fourth {{IEEE International Conference}} on {{Computer Vision Systems}} ({{ICVS}}'06)},
  isbn = {9780769525068},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/CUF976BL/Scaramuzza et al. - 2006 - A Flexible Technique for Accurate Omnidirectional .pdf}
}

@article{scaramuzzaOmnidirectionalVisionCalibration2007,
  title = {Omnidirectional {{Vision}}: {{From Calibration}} to {{Root Motion Estimation}}},
  author = {Scaramuzza, Davide},
  date = {2007-01-01},
  doi = {10.3929/ethz-a-005567197},
  abstract = {For mobile robots to be able to work with and for people and thus operatein our everyday environments, they need to be able to acquire knowledge through perception. In other words they need to collect sensor measure- ments from which they extract meaningful information. This thesis covers some of the essential components of a robot perception system combining omnidirectional vision, odometry, and 3D laser range finders, from modeling to extrinsic calibration, from feature extraction to ego-motion estimation. We covers all these topics from the “point of view” of an omnidirectional camera. The contributions of this work are several and are listed here. The thesis starts with an overview of the geometry of central omnidirectional cameras and gives also an overview of previous calibration methods. The contributions of this section are three. The first two are a new generalized model for describing both dioptric and catadioptric cameras and a calibration method which takes advantage of planar grids shown around the cameras, like the method in use for standard perspective cameras. The third contribution is the implementation of a toolbox for Matlab (called OCamCalib and freely available on-line), which implements the proposed calibration procedure. The second part of the thesis is dedicated to the extraction and matching of vertical features from omnidirectional images. Vertical features are usually very predominant in indoor and outdoor structured environments and can then be very useful for robot navigation. The contribution of this part is a new method for matching vertical lines. The proposed method takes ad-vantage of a descriptor that is very distinctive for each feature. Furthermore, this descriptor is invariant to rotation and slight changes of illumination. The third part of the thesis is devoted to the extrinsic calibration of an omnidirectional camera with the odometry (i.e. wheel encoders) of a mobile robot. The contribution of this part is a new method of automatic self-iii calibration while the robot is moving. The method is based on an extended Kalman filter that combines the encoder readings with the bearing angle observations of one ore more vertical features in the environment. Furthermore, an example of robot motion estimation is shown using the so calibrated camera-odometry system. The fourth part of the thesis is dedicated to the extrinsic calibration of an omnidirectional camera with a 3D laser range finder. The contribution of this method is that it uses no calibration object. Conversely, calibration is performed using laser-camera correspondences of natural points that are manually selected by the user. The novelty of the method resides in a new technique to visualize the usually ambiguous 3D information of range finders. We show that is possible to transform the range information into a new image where natural features of the environment are highlighted. Therefore, finding laser-camera correspondences becomes as easy as image pairing. The last part of the thesis is devoted to visual odometry for outdoor ground vehicles. We show a new method to recover the trajectory of a calibrated omnidirectional camera over several hundred of meters by combining a feature based with an appearance based approach. All the contributions of this thesis are validated through experimental results using both simulated and real data.},
  keywords = {notion},
  annotation = {MAG ID: 2802833655 S2ID: d5c3fad15a58cf881a0fa38e331f14d04e09840b}
}

@inproceedings{scaramuzzaToolboxEasilyCalibrating2006,
  title = {A {{Toolbox}} for {{Easily Calibrating Omnidirectional Cameras}}},
  author = {Scaramuzza, Davide and Martinelli, Agostino and Siegwart, Roland},
  date = {2006-10-01},
  doi = {10.1109/IROS.2006.282372},
  abstract = {In this paper, we present a novel technique for calibrating central omnidirectional cameras. The proposed procedure is very fast and completely automatic, as the user is only asked to collect a few images of a checker board, and click on its corner points. In contrast with previous approaches, this technique does not use any specific model of the omnidirectional sensor. It only assumes that the imaging function can be described by a Taylor series expansion whose coefficients are estimated by solving a four-step least-squares linear minimization problem, followed by a non-linear refinement based on the maximum likelihood criterion. To validate the proposed technique, and evaluate its performance, we apply the calibration on both simulated and real data. Moreover, we show the calibration accuracy by projecting the color information of a calibrated camera on real 3D points extracted by a 3D sick laser range finder. Finally, we provide a Toolbox which implements the proposed calibration procedure.},
  eventtitle = {{{IEEE International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  keywords = {notion,Read},
  file = {/home/astadnik/Zotero/storage/HNNYD54C/Scaramuzza et al. - 2006 - A Toolbox for Easily Calibrating Omnidirectional C.pdf}
}

@article{schaffalitzkyGeometricGroupingRepeated1998,
  title = {Geometric {{Grouping}} of {{Repeated Elements}} within {{Images}}},
  author = {Schaffalitzky, Frederik and Zisserman, Andrew},
  date = {1998-09-18},
  journaltitle = {In Shape, Contour and Grouping in Computer Vision LNCS},
  shortjournal = {In Shape, Contour and Grouping in Computer Vision LNCS},
  volume = {1681},
  issn = {978-3-540-66722-3},
  doi = {10.5244/C.12.2},
  abstract = {The objective of this work is the automatic detection and grouping of imaged elements which repeat on a plane in a scene (for example tiled floorings). It is shown that structures that repeat on a scene plane are related by particular parametrized transformations in perspective images. These image transformations provide powerfulgrou ping constraints, and can be used at the heart of hypothesize and verify grouping algorithms. The parametrized transformations are global across the image plane and may be computed without knowledge of the pose of the plane or camera calibration. Parametrized transformations are given for severalcl asses of repeating operation in the world as well as groupers based on these. These groupers are demonstrated on a number of realim ages, where both the elements and the grouping are determined automatically. It is shown that the repeating element can be learnt from the image, and hence provides an image descriptor. Also, information on the plane pose, such as its vanishing line, can be recovered from the grouping.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/J688NC2E/Schaffalitzky and Zisserman - 1998 - Geometric Grouping of Repeated Elements within Ima.pdf}
}

@online{schopsWhyHaving102020,
  title = {Why {{Having}} 10,000 {{Parameters}} in {{Your Camera Model}} Is {{Better Than Twelve}}},
  author = {Schöps, Thomas and Larsson, Viktor and Pollefeys, Marc and Sattler, Torsten},
  date = {2020-06-23},
  eprint = {1912.02908},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1912.02908},
  url = {http://arxiv.org/abs/1912.02908},
  urldate = {2023-01-05},
  abstract = {Camera calibration is an essential first step in setting up 3D Computer Vision systems. Commonly used parametric camera models are limited to a few degrees of freedom and thus often do not optimally fit to complex real lens distortion. In contrast, generic camera models allow for very accurate calibration due to their flexibility. Despite this, they have seen little use in practice. In this paper, we argue that this should change. We propose a calibration pipeline for generic models that is fully automated, easy to use, and can act as a drop-in replacement for parametric calibration, with a focus on accuracy. We compare our results to parametric calibrations. Considering stereo depth estimation and camera pose estimation as examples, we show that the calibration error acts as a bias on the results. We thus argue that in contrast to current common practice, generic models should be preferred over parametric ones whenever possible. To facilitate this, we released our calibration pipeline at https://github.com/puzzlepaint/camera\_calibration, making both easy-to-use and accurate camera calibration available to everyone.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion,Read},
  file = {/home/astadnik/Zotero/storage/JZRSAH4C/Schöps et al. - 2020 - Why Having 10,000 Parameters in Your Camera Model .pdf;/home/astadnik/Zotero/storage/TBIR7CZL/1912.html}
}

@book{schubertTUMVIBenchmark2018,
  title = {The {{TUM VI Benchmark}} for {{Evaluating Visual-Inertial Odometry}}},
  author = {Schubert, David and Goll, Thore and Demmel, Nikolaus and Usenko, Vladyslav and Stückler, Jörg and Cremers, Daniel},
  date = {2018-10-01},
  pages = {1687},
  doi = {10.1109/IROS.2018.8593419},
  pagetotal = {1680},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/2KCTM8U8/Schubert et al. - 2018 - The TUM VI Benchmark for Evaluating Visual-Inertia.pdf}
}

@article{shiGoodFeaturesTrack2000,
  title = {Good {{Features}} to {{Track}}},
  author = {Shi, Jianbo and Tomasi, Carlo},
  date = {2000-03-03},
  journaltitle = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  shortjournal = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  volume = {600},
  doi = {10.1109/CVPR.1994.323794},
  abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments. 1 Introduction IEEE Conference on Computer Vision and Pattern Recognition (CVPR94) Seattle, June 1994 Is feature tracking a solved problem? The extensive studies of image correlation [4], [3], [15], [18], [7], [17] and sum-of-squared-difference (SSD...},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/9EUW4RVL/Shi and Tomasi - 2000 - Good Features to Track.pdf}
}

@inproceedings{sturmGenericConceptCamera2004,
  title = {A {{Generic Concept}} for {{Camera Calibration}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2004},
  author = {Sturm, Peter and Ramalingam, Srikumar},
  editor = {Pajdla, Tomás and Matas, Jiří},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--13},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24671-8_1},
  abstract = {We present a theory and algorithms for a generic calibration concept that is based on the following recently introduced general imaging model. An image is considered as a collection of pixels, and each pixel measures the light travelling along a (half-) ray in 3-space associated with that pixel. Calibration is the determination, in some common coordinate system, of the coordinates of all pixels’ rays. This model encompasses most projection models used in computer vision or photogrammetry, including perspective and affine models, optical distortion models, stereo systems, or catadioptric systems – central (single viewpoint) as well as non-central ones. We propose a concept for calibrating this general imaging model, based on several views of objects with known structure, but which are acquired from unknown viewpoints. It allows in principle to calibrate cameras of any of the types contained in the general imaging model using one and the same algorithm. We first develop the theory and an algorithm for the most general case: a non-central camera that observes 3D calibration objects. This is then specialized to the case of central cameras and to the use of planar calibration objects. The validity of the concept is shown by experiments with synthetic and real data.},
  isbn = {978-3-540-24671-8},
  langid = {english},
  keywords = {Bundle Adjustment,Calibration Point,Camera Calibration,Camera Model,notion,Optical Center},
  file = {/home/astadnik/Zotero/storage/7QCI57ME/Sturm and Ramalingam - 2004 - A Generic Concept for Camera Calibration.pdf}
}

@incollection{triggsBundleAdjustmentModern2000,
  title = {Bundle {{Adjustment}} — {{A Modern Synthesis}}},
  booktitle = {Vision {{Algorithms}}: {{Theory}} and {{Practice}}},
  author = {Triggs, Bill and McLauchlan, Philip F. and Hartley, Richard I. and Fitzgibbon, Andrew W.},
  editor = {Triggs, Bill and Zisserman, Andrew and Szeliski, Richard},
  editorb = {Goos, Gerhard and Hartmanis, Juris and family=Leeuwen, given=Jan, prefix=van, useprefix=true},
  editorbtype = {redactor},
  date = {2000},
  volume = {1883},
  pages = {298--372},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44480-7_21},
  url = {https://link.springer.com/10.1007/3-540-44480-7_21},
  urldate = {2023-02-16},
  abstract = {This paper is a survey of the theory and methods of photogrammetric bundle adjustment, aimed at potential implementors in the computer vision community. Bundle adjustment is the problem of refining a visual reconstruction to produce jointly optimal structure and viewing parameter estimates. Topics covered include: the choice of cost function and robustness; numerical optimization including sparse Newton methods, linearly convergent approximations, updating and recursive methods; gauge (datum) invariance; and quality control. The theory is developed for general robust cost functions rather than restricting attention to traditional nonlinear least squares.},
  isbn = {978-3-540-67973-8 978-3-540-44480-0},
  langid = {english},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/VQS84R7B/Triggs et al. - 2000 - Bundle Adjustment — A Modern Synthesis.pdf}
}

@book{usenkoDoubleSphereCamera2018,
  title = {The {{Double Sphere Camera Model}}},
  author = {Usenko, Vladyslav and Demmel, Nikolaus and Cremers, Daniel},
  date = {2018-07-24},
  abstract = {Vision-based motion estimation and 3D reconstruction, which have numerous applications (e.g., autonomous driving, navigation systems for airborne devices and augmented reality) are receiving significant research attention. To increase the accuracy and robustness, several researchers have recently demonstrated the benefit of using large field-of-view cameras for such applications. In this paper, we provide an extensive review of existing models for large field-of-view cameras. For each model we provide projection and unprojection functions and the subspace of points that result in valid projection. Then, we propose the Double Sphere camera model that well fits with large field-of-view lenses, is computationally inexpensive and has a closed-form inverse. We evaluate the model using a calibration dataset with several different lenses and compare the models using the metrics that are relevant for Visual Odometry, i.e., reprojection error, as well as computation time for projection and unprojection functions and their Jacobians. We also provide qualitative results and discuss the performance of all models.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/87XIDBEP/Usenko et al. - 2018 - The Double Sphere Camera Model.pdf}
}

@article{v.douskosAutomaticCalibrationDigital2007,
  title = {Automatic {{Calibration}} of {{Digital Cameras Using Planar Chessboard Patterns}}},
  author = {{V. Douskos} and {I. Kalisperakis} and {G. Karras}},
  date = {2007},
  abstract = {A variety of methods for camera calibration, relying on different camera models, algorithms and a priori object information, have been reported and reviewed in literature. Use of simple 2D patterns of the chess-board type represents an interesting approach, for which several ‘calibration toolboxes’ are available on the Internet, requiring varying degrees of human interaction. This paper presents an automatic multi-image approach exclusively for camera calibration purposes on the assumption that the imaged pattern consists of adjacent light and dark squares of equal size. Calibration results, also based on image sets from Internet sources, are viewed as satisfactory and comparable to those from other approaches. Questions regarding the role of image configuration need further investigation.},
  keywords = {notion},
  annotation = {MAG ID: 2908202771 S2ID: 756a80cac6ff6fd03cb0cf5f5cf8de70b12bb403}
}

@article{wangImageQualityAssessment2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  date = {2004-04},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Data mining,Degradation,Humans,Image quality,Indexes,Layout,notion,Quality assessment,Transform coding,Visual perception,Visual system},
  file = {/home/astadnik/Zotero/storage/P4UFJ2VD/1284395.html}
}

@article{xueFisheyeDistortionRectification2020,
  title = {Fisheye {{Distortion Rectification}} from {{Deep Straight Lines}}},
  author = {Xue, Zhucun and Xue, Nan and Xia, Guisong},
  date = {2020-03-25},
  journaltitle = {ArXiv},
  url = {https://www.semanticscholar.org/paper/Fisheye-Distortion-Rectification-from-Deep-Straight-Xue-Xue/c3d3715a0ab01dcf40c35e9be7f9facd5e122284},
  urldate = {2023-01-30},
  abstract = {This paper presents a novel line-aware rectification network (LaRecNet) to address the problem of fisheye distortion rectification based on the classical observation that straight lines in 3D space should be still straight in image planes. Specifically, the proposed LaRecNet contains three sequential modules to (1) learn the distorted straight lines from fisheye images; (2) estimate the distortion parameters from the learned heatmaps and the image appearance; and (3) rectify the input images via a proposed differentiable rectification layer. To better train and evaluate the proposed model, we create a synthetic line-rich fisheye (SLF) dataset that contains the distortion parameters and well-annotated distorted straight lines of fisheye images. The proposed method enables us to simultaneously calibrate the geometric distortion parameters and rectify fisheye images. Extensive experiments demonstrate that our model achieves state-of-the-art performance in terms of both geometric accuracy and image quality on several evaluation metrics. In particular, the images rectified by LaRecNet achieve an average reprojection error of 0.33 pixels on the SLF dataset and produce the highest peak signal-to-noise ratio (PSNR) and structure similarity index (SSIM) compared with the groundtruth.},
  keywords = {notion},
  file = {/home/astadnik/Zotero/storage/2S69I6PH/Xue et al. - 2020 - Fisheye Distortion Rectification from Deep Straigh.pdf}
}

@article{zhangFlexibleNewTechnique2000,
  title = {A Flexible New Technique for Camera Calibration},
  author = {Zhang, Z.},
  date = {2000-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {22},
  number = {11},
  pages = {1330--1334},
  issn = {1939-3539},
  doi = {10.1109/34.888718},
  abstract = {We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Calibration,Cameras,Closed-form solution,Computer simulation,Computer vision,Layout,Lenses,Maximum likelihood estimation,Nonlinear distortion,notion,Testing},
  file = {/home/astadnik/Zotero/storage/MVQE9NE7/Zhang - 2000 - A Flexible New Technique for Camera Calibration.pdf;/home/astadnik/Zotero/storage/9YUDAWJG/888718.html}
}

@software{zhangSnowball2023,
  title = {Snowball},
  author = {Zhang, Shengchen},
  date = {2023-01-06T20:30:30Z},
  origdate = {2022-05-30T13:14:21Z},
  url = {https://github.com/shaunabanana/snowball},
  urldate = {2023-01-11},
  abstract = {Find and filter literature, fast.},
  keywords = {notion}
}
